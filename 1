5.3 完整视频分割示例
以下是一个完整的视频分割和可视化示例。采用逐帧处理的方式，避免一次性加载所有帧导致CUDA内存溢出（OOM）：

from sam3.model_builder import build_sam3_video_predictor
from sam3.visualization_utils import (
    prepare_masks_for_visualization,
    visualize_formatted_frame_output,
)
import cv2
import numpy as np
import matplotlib.pyplot as plt
from io import BytesIO
from PIL import Image

# 设置matplotlib后端为Agg，避免显示问题
plt.switch_backend('Agg')

# 模型路径
MODEL_PATH = "/root/.cache/modelscope/hub/models/facebook/sam3/sam3.pt"

def extract_video_frames(video_path):
    """提取视频帧用于可视化"""
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame_rgb)
    cap.release()
    return frames

def create_visualization_frame(frame_idx, video_frames, outputs, target_size):
    """创建可视化帧"""
    try:
        fig = plt.figure(figsize=(target_size[0]/100, target_size[1]/100), dpi=100)
        
        visualize_formatted_frame_output(
            frame_idx,
            video_frames,
            outputs_list=[prepare_masks_for_visualization({frame_idx: outputs})],
            titles=["SAM 3 Tracking"],
        )
        
        plt.tight_layout(pad=0)
        plt.axis('off')
        
        buf = BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', 
                   pad_inches=0, dpi=100, facecolor='black')
        buf.seek(0)
        
        img = Image.open(buf).convert('RGB')
        img_array = np.array(img)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        
        plt.close(fig)
        buf.close()
        
        if img_bgr.shape[1] != target_size[0] or img_bgr.shape[0] != target_size[1]:
            img_bgr = cv2.resize(img_bgr, target_size)
            
        return img_bgr
        
    except Exception as e:
        print(f"创建可视化帧 {frame_idx} 时出错: {e}")
        return np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)

# 初始化预测器
video_predictor = build_sam3_video_predictor(checkpoint_path=MODEL_PATH)
video_path = "./test_video.mp4"

# 开始会话
response = video_predictor.handle_request(
    request=dict(type="start_session", resource_path=video_path)
)
session_id = response["session_id"]
print(f"会话已创建: {session_id}")

# 获取视频信息
cap = cv2.VideoCapture(video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
cap.release()
print(f"视频信息: {width}x{height}, {fps} FPS, 总帧数: {total_frames}")

# 提取视频帧用于可视化
video_frames_for_vis = extract_video_frames(video_path)
print(f"提取了 {len(video_frames_for_vis)} 帧")

# 逐帧处理：为每一帧添加提示并获取分割结果（关键：避免内存溢出）
print("开始逐帧处理...")
all_outputs = {}

for frame_idx in range(total_frames):
    try:
        response = video_predictor.handle_request(
            request=dict(
                type="add_prompt",
                session_id=session_id,
                frame_index=frame_idx,
                text="person",  # 要追踪的目标
            )
        )
        
        if "outputs" in response:
            all_outputs[frame_idx] = response["outputs"]
            print(f"成功处理帧 {frame_idx + 1}/{total_frames}")
        else:
            print(f"帧 {frame_idx} 没有输出结果")
            
    except Exception as e:
        print(f"处理帧 {frame_idx} 时出错: {e}")
        break

print(f"成功处理了 {len(all_outputs)} 帧")

# 创建视频写入器
output_video_path = "segmentation_video.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

# 生成可视化视频
print("开始生成可视化视频...")
processed_count = 0
for frame_idx in range(total_frames):
    if frame_idx in all_outputs:
        try:
            vis_frame = create_visualization_frame(
                frame_idx, video_frames_for_vis, all_outputs[frame_idx], (width, height)
            )
            out_video.write(vis_frame)
            processed_count += 1
            print(f"视频生成进度: {frame_idx + 1}/{total_frames}")
        except Exception as e:
            print(f"生成帧 {frame_idx} 时出错: {e}")
            black_frame = np.zeros((height, width, 3), dtype=np.uint8)
            out_video.write(black_frame)
    else:
        black_frame = np.zeros((height, width, 3), dtype=np.uint8)
        out_video.write(black_frame)

out_video.release()
print(f"视频生成完成，成功处理 {processed_count} 帧")
print(f"分割视频已保存到: {output_video_path}")

# 结束会话（重要：释放GPU内存）
video_predictor.handle_request(
    request=dict(type="close_session", session_id=session_id)
)
video_predictor.shutdown()